{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Utils\n",
        "## Import libraries"
      ],
      "metadata": {
        "id": "hRRSbOAqXcK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2960rEAWXOsB"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as tfk\n",
        "from tensorflow.keras import utils, layers, losses, optimizers, models, Sequential, Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "eMdZWdglXmIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
        "    '''\n",
        "       Description: split data to x_train, y_train, x_valid, y_valid\n",
        "       *Args: \n",
        "       images: images as a array\n",
        "       labels: labels as a array\n",
        "\n",
        "       *Returns: \n",
        "       x_train: images using in training\n",
        "       x_valid: images using in validation\n",
        "       y_train: labels of training images\n",
        "       y_valid: labels of validation images\n",
        "\n",
        "    '''\n",
        "    # 1. Get the total size of the dataset\n",
        "    size = len(images)\n",
        "    # 2. Make an indices array and shuffle it, if required\n",
        "    indices = np.arange(size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    # 3. Get the size of training samples\n",
        "    train_samples = int(size * train_size)\n",
        "    # 4. Split data into training and validation sets\n",
        "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
        "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "\n",
        "def preprocess(img_path, label):\n",
        "    '''\n",
        "       Description: is preprocessing function which will be used in creating dataset.\n",
        "                     it contains of reading image, convert to grayscale image float32, resize it \n",
        "                     to desired size and transpose(because we want the time-dimension to correspond to the width of the image) \n",
        "       *Args: \n",
        "       img_path: path of image to read it\n",
        "       label: labels\n",
        "\n",
        "       *Returns:\n",
        "       {\"image\": img, \"label\": label}: a dict as our model is expecting two inputs\n",
        "\n",
        "\n",
        "    '''\n",
        "    # Read image\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # Decode and convert to grayscale\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    # Convert to float32 in [0, 1] range\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # Resize to the desired size\n",
        "    img = tf.image.resize(img, [50, 200])\n",
        "    # Transpose the image because we want the time-dimension to correspond to the width of the image.\n",
        "    img = tf.transpose(img, perm=[1, 0, 2])\n",
        "    # Map the characters in label to numbers\n",
        "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
        "    # Return a dict as our model is expecting two inputs\n",
        "    return {\"image\": img, \"label\": label}\n",
        "\n",
        "\n",
        "def create_dataset(x_data, y_data, batch_size):\n",
        "    '''\n",
        "      Description: is a utility function to create a dataset(As tensorflow.python.data.ops.dataset_ops.PrefetchDataset Class)\n",
        "                    which will be prepared for training step(it contains preprocess(mapped by preprocess function), batch, prefetch)\n",
        "      *Args:\n",
        "      x_data: CAPTCHA images (for training and validation step)\n",
        "      y_data: text of correspondent CAPTCHA images\n",
        "      batch_size: batch size\n",
        "\n",
        "      *Returns:\n",
        "      dataset: As tensorflow.python.data.ops.dataset_ops.PrefetchDataset Class\n",
        "\n",
        "    '''\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
        "    dataset = (dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "    return dataset \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sGpAHUvrXPIf"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}